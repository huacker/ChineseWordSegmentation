# ChineseWordSegmentation
实现了两大类中文分词算法，分别是基于词典匹配的分词算法和基于CNN的分词算法
一、基于词典匹配的分词算法

最大匹配算法 Maximum WordSegWithMatch
{
    正向最大匹配 MaxMatch,
    逆向最大匹配 ReverseMaxMatch,
    双向最大匹配 BothwayMaxMatch;
}

分词算法设计中的几个基本原则：

1、颗粒度越大越好。
用于进行语义分析的文本分词，要求分词结果的颗粒度越大，即单词的字数越多，所能表示的含义越确切。
如：“公安局长”可以分为“公安 局长”、“公安局 长”、“公安局长”，都算对，但是要用于语义分析，则“公安局长”的分词结果最好（当然前提是所使用的词典中有这个词）

2、切分结果中非词典词越少越好，单字字典词数越少越好。
这里的“非词典词”就是不包含在词典中的单字，而“单字字典词”指的是可以独立运用的单字，如“的”、“了”、“和”、“你”、“我”、“他”。
例如：“技术和服务”，可以分为“技术 和服 务”以及“技术 和 服务”，
因为“务”字无法独立成词（即词典中没有），而“和”字可以单独成词（词典中要包含），因此“技术 和服 务”有1个非词典词，而“技术 和 服务”有0个非词典词，因此选用后者。

3、总体词数越少越好。
在相同字数的情况下，总词数越少，说明语义单元越少，那么相对的单个语义单元的权重会越大，因此准确性会越高。


运行环境：
python 3.6
运行配置：
新建word_dict.txt文件，路径配置到word_seg_with_match.py文件中
运行：
python word_seg_with_match.py
